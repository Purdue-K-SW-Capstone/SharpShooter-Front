{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87813226",
   "metadata": {},
   "source": [
    "# Demo Code with Face and Hand Recognition\n",
    "\n",
    "This code is for running the simple demo (prototype) of the system with face and hand recognition models.  \n",
    "If you can't run another demo code, this is the easiest way to see a result.  \n",
    "There are some libraries to run this code on the first cell, so please check it.  \n",
    "It is recommend to run the code with Jupyter Notebook. Run each cell in order.  \n",
    "\n",
    "- Recommend IDE: Jupyter Notebook, Visual Sutdio Code\n",
    "- Language: **Python 3.10**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b750d",
   "metadata": {},
   "source": [
    "## 1. Import and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aeb8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "\n",
    "# For models\n",
    "import face_recognition\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0427fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtang\\Desktop\\SW\\@KSW Gangsture\\dolphin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtang\\anaconda3\\envs\\mp\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\mtang\\anaconda3\\envs\\mp\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\mtang\\anaconda3\\envs\\mp\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator PCA from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "# Cofiguration: set the specific model name and the root\n",
    "path = os.getcwd() # current path\n",
    "print(path)        # for checking \n",
    "\n",
    "# For hand recognition model (XGB Classifier)\n",
    "model_xgb = pickle.load(open(path + '/Model/HAND/1025model_xgb.h5', 'rb'))   # config\n",
    "\n",
    "# For face recognition model (PCA and SVM)\n",
    "model_svm = pickle.load(open(path + '/Model/FACE/1207_svm_20064.sav', 'rb')) # config\n",
    "model_pca = pickle.load(open(path+'/Model/FACE/1207_pca_20064.pkl', 'rb'))   # config\n",
    "# Import haar cascade classifier from cv2 \n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c546b1",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions\n",
    "\n",
    "- detect_face\n",
    "- calc_relative_coord\n",
    "- preprocess_facedata\n",
    "- preprocess_handdata\n",
    "- calc_bounding_rect\n",
    "- draw_bounding_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c012cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Two functions for detecting and calculating information #####\n",
    "# Detect face on the image and return coordinates \n",
    "def detect_face(gray_img, face_cascade = face_cascade, width=200, height=200) :\n",
    "    # Setting detector function \n",
    "    coordinate = face_cascade.detectMultiScale( \n",
    "            gray_img,          # input grayscale image \n",
    "            scaleFactor=1.3,   # imgae pyramid scale \n",
    "            minNeighbors=4)    # neighbor object minimum distance pixels \n",
    "    \n",
    "    # Detect face coordinate location in image, i.e, detect face\n",
    "    # If the face was not detected, \n",
    "    if coordinate is None or len(coordinate)==0 : \n",
    "        return None\n",
    "    else : \n",
    "        # If faces were detected (two more), \n",
    "        if len(coordinate) == 2 :\n",
    "            x1,y1,w,h = coordinate[-1].squeeze()\n",
    "        # If just one face was detected,\n",
    "        else : \n",
    "            x1,y1,w,h = coordinate[0]        \n",
    "        x1, y1 = abs(x1), abs(y1)\n",
    "        x2 = abs(x1+w)\n",
    "        y2 = abs(y1+h)\n",
    "        \n",
    "    face = gray_img[y1:y2, x1:x2]\n",
    "    face = cv2.resize(face, (width, height))\n",
    "    \n",
    "    return face\n",
    "\n",
    "# Calculate and Create the relative coordinates from the original one\n",
    "def calc_relative_coord(landmarks) :\n",
    "    lms_df = pd.DataFrame(landmarks).drop(0, axis=1)\n",
    "    relative_list = []\n",
    "    \n",
    "    base_lm = lms_df.iloc[0, 1:].values.astype(float)\n",
    "    for i in range(0, 21):\n",
    "        target_lm = lms_df.loc[i, 2:].values.astype(float)\n",
    "        result_lm = target_lm - base_lm\n",
    "        relative_list.append(result_lm)\n",
    "    \n",
    "    relative_df = pd.DataFrame(relative_list)\n",
    "    new_df = pd.concat([lms_df, relative_df], axis=1)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "##### Two functions for preprocessing face and hand data as input data #####\n",
    "# Preprocess for hand dataset\n",
    "def preprocess_handdata(data) :\n",
    "    # store hand_type value\n",
    "    hand_type = data.iloc[0, 0] \n",
    "    \n",
    "    # remove hand_type value column \n",
    "    data = data.iloc[:, 1:].copy()\n",
    "    \n",
    "    # converting to numpy array \n",
    "    data_np = np.array(data)\n",
    "    data_np = data_np.flatten()\n",
    "    \n",
    "    # insert hand_type value \n",
    "    if hand_type == \"right\" :\n",
    "        data_np = np.insert(data_np, 0, 0)\n",
    "    else :\n",
    "        data_np = np.insert(data_np, 0, 1)\n",
    "    \n",
    "    # re-shape for input data format of trained model \n",
    "    input_data = data_np.reshape(1, data_np.shape[0])\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# Preprocess for face dataset\n",
    "def preprocess_facedata(data, pca=model_pca) :\n",
    "    data = data.flatten().astype('int').reshape(1,-1)\n",
    "    input_data = pca.transform(data)\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "\n",
    "##### Two functions for bounding rectangle #####\n",
    "# Calculate bounding rectangle\n",
    "def calc_bounding_rect(image, landmarks):  \n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "# Draw bounding rectangle \n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721e36c",
   "metadata": {},
   "source": [
    "## 3. Start Collecting Face Dataset\n",
    "The simple demo code to test the entire system with web cam  \n",
    "Originally, this demo will show the face label predicted by recognition machine learning model,  \n",
    "**but it was changed to be available when the face is recognized for testing smoothly.**   \n",
    "So if the face is detected, the hand will be recognized. The original codes are also written by comments. Please note these changes. \n",
    "- to quit the code, press 'q' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b44388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for Mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1,              # Only detect one hand \n",
    "                       min_detection_confidence=0.7) # defualt 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam \n",
    "video_capture = cv2.VideoCapture(0) # 0 is a default embedded camera\n",
    "\n",
    "# Initialize for dataset\n",
    "landmarks = []\n",
    "df = pd.DataFrame()\n",
    "flag = False # If the flag is False, hand recognition is not working \n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = video_capture.read()\n",
    "    x, y, c = frame.shape\n",
    "    \n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)   # For hand\n",
    "    framegray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # For face\n",
    "    \n",
    "    \n",
    "    # Detect and extract face from gray frame \n",
    "    coords = detect_face(framegray)\n",
    "    \n",
    "    # Preprocessing for input face data \n",
    "    if coords is not None: \n",
    "        flag = True\n",
    "        # Input data for face recognition\n",
    "        input_face = preprocess_facedata(coords)\n",
    "        pred_proba_face = model_svm.predict_proba(input_face)\n",
    "        # If the probability of detected face was lower than threshold \n",
    "#         if np.max(pred_proba_face) <= 0.5 : # config: threshold is 0.5\n",
    "#             flag = False \n",
    "#         # Predict face input\n",
    "#         pred_face = np.argmax(pred_proba_face)\n",
    "    # If face is not detected\n",
    "    else: \n",
    "        flag = False \n",
    "    \n",
    "    \n",
    "    # Check the authority with \"flag\" status, Can we use the gesture controller? \n",
    "    if flag == True :\n",
    "        # Show the autorithy with face prediction label \n",
    "#         cv2.putText(frame, \"Accessed! with face label \"+str(pred_face), (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"Accessed!\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Mediapipe processing\n",
    "        result = hands.process(framergb)\n",
    "        landmarks.clear() # For empty list \n",
    "        if result.multi_hand_landmarks:\n",
    "            for handslms, handness in zip(result.multi_hand_landmarks,  # Be carefule to set attribute for right coordinate system\n",
    "                                          result.multi_handedness): \n",
    "                for point in mp_hands.HandLandmark: # 0 ~ 20\n",
    "                    x = handslms.landmark[point].x\n",
    "                    y = handslms.landmark[point].y\n",
    "                    z = handslms.landmark[point].z\n",
    "                    landmarks.append([str(point), handness.classification[0].label, x, y, z])\n",
    "\n",
    "                # Draw landmarks on frames with bounding rectangle\n",
    "                brect = calc_bounding_rect(frame, handslms)\n",
    "                frame = draw_bounding_rect(True, frame, brect)\n",
    "                mp_drawing.draw_landmarks(frame, handslms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Calculate realtive coordinates with hand landmarks \n",
    "        if landmarks:\n",
    "            new_landmarks = calc_relative_coord(landmarks)\n",
    "            \n",
    "            # Preprocessing for input hand data\n",
    "            input_hand = preprocess_handdata(new_landmarks.copy())\n",
    "\n",
    "            # Input data for hand recognition\n",
    "            pred_proba_hand = model_xgb.predict_proba(input_hand)\n",
    "            # Predict hand input \n",
    "            pred_hand = np.argmax(pred_proba_hand)\n",
    "            cv2.putText(frame, \"Your gesture is number \" + str(pred_hand), (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    # To quit from application, press \"q\"\n",
    "    if cv2.waitKey(1) == ord('q'): \n",
    "        break\n",
    "        \n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame)\n",
    "    \n",
    "# Release the webcam and destroy all active windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "mp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
